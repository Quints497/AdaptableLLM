import logging
from src.abstract.base_adapter import BaseAdapter
from llama_cpp import Llama


class Adapter(BaseAdapter):
    """
    Adapter class that interfaces with the Llama model for generating responses based on provided prompts.

    Attributes:
        llm (Llama): Instance of the Llama model for response generation.
        prompt_template (str): Template string for formatting prompts.
        generating (bool): Flag indicating if response generation is in progress.
        logger (logging.Logger): Logger for recording adapter operations.
    """

    def __init__(self, model_path: str, n_gpu_layers: int, n_batch: int, n_ctx: int, verbose: bool, prompt_template: str) -> None:
        """
        Initializes the Adapter with Llama model parameters and a prompt template.

        Args:
            model_path (str): File system path to the pretrained model.
            n_gpu_layers (int): Number of layers to run on the GPU.
            n_batch (int): Number of tokens to process in parallel.
            n_ctx (int): Maximum number of tokens the model can consider for a single prompt.
            verbose (bool): Enables detailed logging if True.
            prompt_template (str): Template for generating prompts, with placeholders for dynamic content.
        """
        self.llm = Llama(model_path=model_path, n_gpu_layers=n_gpu_layers, n_batch=n_batch, n_ctx=n_ctx, verbose=verbose)
        self.prompt_template = prompt_template
        self.generating = False
        self.__name__ = "Adapter"
        self.init_logging()

    def init_logging(self) -> None:
        """
        Sets up logging configuration for the adapter, including file handlers and formatting.
        """
        self.logger = logging.getLogger("Adapter-Logger")
        self.logger.setLevel(logging.INFO)
        handler = logging.FileHandler(f"{__name__}-{self.__name__}.log")
        formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)

        # Prevent adding duplicate handlers
        if not self.logger.hasHandlers():
            self.logger.addHandler(handler)

    def invoke(self, prompt: str, **parameters) -> str:
        """
        Generates a response from the Llama model based on the input prompt and additional parameters.

        Args:
            prompt (str): Input prompt for the model.
            **parameters: Variable keyword arguments for model configuration.

        Returns:
            str: The generated response from the model.
        """
        self.generating = True
        self.logger.info(f"Generating: {self.generating}")
        output = self.llm(prompt, **parameters)
        self.generating = False
        self.logger.info(f"Generating: {self.generating}")
        return output

    def prompt_format(self, prompt: str) -> str:
        """
        Formats the input prompt using the predefined template.

        Args:
            prompt (str): The user's input prompt.

        Returns:
            str: Formatted prompt ready for model processing.
        """
        return self.prompt_template.format(prompt=prompt)

    def parse_response(self, output):
        """
        Parses and yields each chunk of the generated response.

        Args:
            output: The raw output generated by the Llama model.

        Yields:
            str: Each parsed chunk of the response.
        """
        for idx, chunk in enumerate(output):
            if idx == 0:  # Skip the first chunk if needed
                continue
            yield chunk['choices'][0]['text']
